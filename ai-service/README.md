# ğŸ¤– AI Microservice

Microservicio de IA **modular y reutilizable** con Ollama/LLama para integrar chat inteligente en cualquier aplicaciÃ³n.

## ğŸ¯ CaracterÃ­sticas

- âœ… **Totalmente independiente** - Corre en su propio puerto
- âœ… **Modular** - Solo cambias `config/contexts.json` para otro proyecto
- âœ… **Conversacional** - Mantiene historial por sesiÃ³n
- âœ… **Contextual** - Puedes pasarle datos especÃ­ficos del caso
- âœ… **Reutilizable** - Usa en forensics, marketplace, blog, etc.

## ğŸ“‹ Requisitos

1. **Node.js** (v18 o superior)
2. **Ollama** instalado y corriendo

### Instalar Ollama

```bash
# Windows/Mac: Descarga desde https://ollama.com/download
# Linux:
curl -fsSL https://ollama.com/install.sh | sh

# Descargar modelo LLama 3.2
ollama pull llama3.2

# Verificar que estÃ© corriendo
ollama serve
```

## ğŸš€ InstalaciÃ³n

```bash
cd ai-service
npm install
```

## âš™ï¸ ConfiguraciÃ³n

Edita `.env` si necesitas cambiar puertos:

```env
PORT=5001                           # Puerto del servicio
OLLAMA_HOST=http://localhost:11434 # URL de Ollama
FRONTEND_URL=http://localhost:5173 # URL del frontend (CORS)
```

## ğŸƒ Ejecutar

```bash
# Desarrollo (con auto-reload)
npm run dev

# ProducciÃ³n
npm start
```

DeberÃ­as ver:

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  ğŸ¤– AI MICROSERVICE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸš€ Servidor corriendo en puerto 5001
ğŸ“¡ API: http://localhost:5001/api
ğŸ¥ Health check: http://localhost:5001/api/health
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

## ğŸ“¡ API Endpoints

### 1. Enviar mensaje al chat

```http
POST /api/chat
Content-Type: application/json

{
  "contextId": "forensic-dna",
  "message": "ExplÃ­came el caso CASO-2024-018",
  "sessionId": "user123_case456",  // Opcional
  "caseData": {                     // Opcional
    "casoNumero": "CASO-2024-018",
    "patrones": ["ATCG", "GCTA"],
    "coincidencias": 15
  }
}
```

**Respuesta:**
```json
{
  "success": true,
  "data": {
    "response": "El caso CASO-2024-018 involucra...",
    "model": "llama3.2",
    "responseTime": 2500,
    "sessionId": "user123_case456"
  }
}
```

### 2. Limpiar historial de sesiÃ³n

```http
DELETE /api/chat/session/:sessionId
```

### 3. Listar contextos disponibles

```http
GET /api/contexts
```

### 4. Health check

```http
GET /api/health
```

## ğŸ”„ Reutilizar en Otro Proyecto

Para usar este microservicio en un proyecto diferente (ej: marketplace):

### 1. Copia la carpeta completa

```bash
cp -r ai-service ../mi-marketplace/ai-service
```

### 2. Edita solo `config/contexts.json`

```json
{
  "marketplace": {
    "name": "Asistente de Marketplace",
    "description": "Ayuda con productos, ventas y soporte",
    "systemPrompt": "Eres un asistente virtual de marketplace...",
    "model": "llama3.2",
    "temperature": 0.8,
    "maxTokens": 300
  }
}
```

### 3. Listo, Ãºsalo

```javascript
// Desde tu frontend/backend
fetch('http://localhost:5001/api/chat', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    contextId: 'marketplace',  // Cambiaste solo esto
    message: 'Â¿CuÃ¡nto cuesta este producto?'
  })
});
```

**NO necesitas cambiar:**
- âŒ `src/server.js`
- âŒ `src/services/ollamaService.js`
- âŒ `src/routes/chatRoutes.js`

**Solo cambias:**
- âœ… `config/contexts.json`
- âœ… `.env` (si cambias puertos)

## ğŸ¨ Ejemplo de Uso (Frontend)

```typescript
// Modo General (chat flotante en todas las pÃ¡ginas)
const respuesta = await fetch('http://localhost:5001/api/chat', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    contextId: 'forensic-dna',
    message: 'Â¿QuÃ© es el algoritmo Aho-Corasick?',
    sessionId: 'user123'
  })
});

// Modo Contextual (botÃ³n "Analizar con IA" en un caso especÃ­fico)
const respuesta = await fetch('http://localhost:5001/api/chat', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    contextId: 'forensic-dna',
    message: 'Â¿QuÃ© tan confiables son estos resultados?',
    sessionId: `user123_${casoId}`,
    caseData: {
      casoNumero: 'CASO-2024-018',
      patrones: ['ATCGATCG', 'GCTAGCTA'],
      totalCoincidencias: 15,
      algoritmoUsado: 'Aho-Corasick',
      tiempoEjecucionMs: 150
    }
  })
});
```

## ğŸ› Troubleshooting

### "Error: connect ECONNREFUSED localhost:11434"

**SoluciÃ³n:** Ollama no estÃ¡ corriendo.

```bash
ollama serve
```

### "Error: Model llama3.2 not found"

**SoluciÃ³n:** Descarga el modelo.

```bash
ollama pull llama3.2
```

### Puerto 5001 ya en uso

**SoluciÃ³n:** Cambia el puerto en `.env`:

```env
PORT=5002
```

## ğŸ“š Contextos Incluidos

- `forensic-dna` - Asistente forense de ADN (anÃ¡lisis, comparaciones, insights)

## ğŸ¯ PrÃ³ximos Pasos

1. **IntegraciÃ³n Frontend:** Crea widget de chat flotante
2. **BotÃ³n Contextual:** Agrega "Analizar con IA" en historial
3. **MÃ¡s Contextos:** Agrega contextos para otros mÃ³dulos

## ğŸ“„ Licencia

MIT - Libre para uso en proyectos comerciales y personales
